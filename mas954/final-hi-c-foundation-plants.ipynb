{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19bf3e12-973e-4bae-9184-35920d20f753",
      "metadata": {},
      "source": [
        "HiCFoundation Resolution Enhancement Pipeline for Google Colab\n",
        "This notebook provides a complete pipeline for Hi-C resolution enhancement using HiCFoundation, optimized for Google Colab.\n",
        "Prerequisites\n",
        "Before starting, make sure to:\n",
        "\n",
        "Enable GPU in Runtime → Change runtime type → Hardware accelerator → GPU (T4 or better)\n",
        "Have your .hic files ready to upload\n",
        "Have a Google Drive account with sufficient storage space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab738a2a-1052-4292-8a74-68686dbdba30",
      "metadata": {},
      "source": [
        "1. Environment Setup\n",
        "Check GPU and Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13085ff7-091a-4efb-b3c2-965d2a32d2c0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "import os\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Please enable GPU in Runtime settings.\")\n",
        "\n",
        "# Mount Google Drive for data storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create working directory in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive/HiCFoundation'\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "os.chdir(DRIVE_PATH)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5b41f3-4fc1-4494-86c1-d4602ee87112",
      "metadata": {},
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f134d3-8ed5-4845-a766-2bd7cd949bab",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install other required packages\n",
        "!pip install easydict opencv-python simplejson lvis Pillow==9.5.0 pytorch_msssim\n",
        "!pip install pandas hic-straw matplotlib scikit-image scipy einops tensorboard\n",
        "!pip install cooler numba pyBigWig timm==0.3.2 scikit-learn\n",
        "\n",
        "# Clone HiCFoundation repositories\n",
        "!git clone https://github.com/Noble-Lab/HiCFoundation.git\n",
        "!git clone https://github.com/Noble-Lab/HiCFoundation_paper.git\n",
        "\n",
        "# Copy necessary files from HiCFoundation repo\n",
        "!cp -r HiCFoundation/* .\n",
        "!cp -r HiCFoundation_paper/utils/* utils/ 2>/dev/null || true"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e757b77a-2671-4311-9252-096658d5c7f9",
      "metadata": {},
      "source": [
        "Create Directory Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1446044-68bb-4a03-9040-d3c589d2410a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create necessary directories\n",
        "dirs_to_create = [\n",
        "    'utils',\n",
        "    'hic-raw',\n",
        "    'input-dirs',\n",
        "    'input-dirs/pre-train-dirs',\n",
        "    'ft-inputs',\n",
        "    'ft-inputs/train',\n",
        "    'ft-inputs/val',\n",
        "    'outputs',\n",
        "    'models',\n",
        "    'logs'\n",
        "]\n",
        "\n",
        "for dir_name in dirs_to_create:\n",
        "    os.makedirs(dir_name, exist_ok=True)\n",
        "    print(f\"Created directory: {dir_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff934b05-97ff-4b77-8768-4043c3a7139b",
      "metadata": {},
      "source": [
        "2. Data Upload\n",
        "Go to this link: https://drive.google.com/drive/folders/1D5MqwauHKRFixhRbGljSnouxWFNVfL1l?usp=sharing\n",
        "Download and Upload the .hic Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96825184-eaa8-4bb7-b016-53d9bec4ca87",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"Upload your .hic files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to hic-raw directory\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f'hic-raw/{filename}')\n",
        "    print(f\"Moved {filename} to hic-raw/\")\n",
        "\n",
        "# List files in hic-raw directory\n",
        "!ls -la hic-raw/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b7d7d5-5442-4036-a6d8-176d11f70d96",
      "metadata": {},
      "source": [
        "3. Data Preprocessing\n",
        "Create hic2array.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4291d455-f54e-447d-b4af-16d5068e897a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%writefile utils/hic2array.py\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "import hicstraw\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "def write_pkl(data, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def read_chrom_array(chr1, chr2, normalization, hic_file, resolution):\n",
        "    chr1_name = chr1.name\n",
        "    chr2_name = chr2.name\n",
        "    infos = []\n",
        "    infos.append('observed')\n",
        "    infos.append(normalization)\n",
        "    infos.append(hic_file)\n",
        "    infos.append(chr1_name)\n",
        "    infos.append(chr2_name)\n",
        "    infos.append('BP')\n",
        "    infos.append(resolution)\n",
        "    print(infos)\n",
        "    row, col, val = [], [], []\n",
        "    rets = hicstraw.straw(*infos)\n",
        "    print('\\tlen(rets): {:3e}'.format(len(rets)))\n",
        "    for ret in rets:\n",
        "        row.append((int)(ret.binX // resolution))\n",
        "        col.append((int)(ret.binY // resolution))\n",
        "        val.append(ret.counts)\n",
        "    print('\\tsum(val): {:3e}'.format(sum(val)))\n",
        "    if sum(val) == 0:\n",
        "        return None\n",
        "    if chr1_name==chr2_name:\n",
        "        max_shape =max(max(row),max(col))+1\n",
        "        mat_coo = coo_matrix((val, (row, col)), shape = (max_shape,max_shape),dtype=np.float32)\n",
        "    else:\n",
        "        max_row = max(row)+1\n",
        "        max_column = max(col)+1\n",
        "        mat_coo = coo_matrix((val, (row, col)), shape = (max_row,max_column),dtype=np.float32)\n",
        "\n",
        "    mat_coo = mat_coo #+ triu(mat_coo, 1).T #no below diagonaline records\n",
        "\n",
        "    return mat_coo\n",
        "\n",
        "\n",
        "def hic2array(input_hic,output_pkl=None,\n",
        "              resolution=25000,normalization=\"NONE\",\n",
        "              tondarray=0):\n",
        "    \"\"\"\n",
        "    input_hic: str, input hic file path\n",
        "    output_pkl: str, output pickle file path\n",
        "    resolution: int, resolution of the hic file\n",
        "    \"\"\"\n",
        "\n",
        "    hic = hicstraw.HiCFile(input_hic)\n",
        "    chrom_list=[]\n",
        "    chrom_dict={}\n",
        "    for chrom in hic.getChromosomes():\n",
        "        print(chrom.name, chrom.length)\n",
        "        if \"all\" in chrom.name.lower():\n",
        "            continue\n",
        "        chrom_list.append(chrom)\n",
        "        chrom_dict[chrom.name]=chrom.length\n",
        "    resolution_list = hic.getResolutions()\n",
        "    if resolution not in resolution_list:\n",
        "        print(\"Resolution not found in the hic file, please choose from the following list:\")\n",
        "        print(resolution_list)\n",
        "        exit()\n",
        "    output_dict={}\n",
        "    for i in range(len(chrom_list)):\n",
        "        for j in range(i,len(chrom_list)):\n",
        "            if i!=j and tondarray in [2,3]:\n",
        "                #skip inter-chromosome region\n",
        "                continue\n",
        "            \n",
        "            chrom1 = chrom_list[i]\n",
        "            chrom1_name = chrom_list[i].name\n",
        "            chrom2 = chrom_list[j]\n",
        "            chrom2_name = chrom_list[j].name\n",
        "            if 'Un' in chrom1_name or 'Un' in chrom2_name:\n",
        "                continue\n",
        "            if \"random\" in chrom1_name.lower() or \"random\" in chrom2_name.lower():\n",
        "                continue\n",
        "            if \"alt\" in chrom1_name.lower() or \"alt\" in chrom2_name.lower():\n",
        "                continue\n",
        "            read_array=read_chrom_array(chrom1,chrom2, normalization, input_hic, resolution)\n",
        "            if read_array is None:\n",
        "                print(\"No data found for\",chrom1_name,chrom2_name)\n",
        "                continue\n",
        "            if tondarray in [1,3]:\n",
        "                read_array = read_array.toarray()\n",
        "            if tondarray in [2,3]:\n",
        "                output_dict[chrom1_name]=read_array\n",
        "            else:\n",
        "                output_dict[chrom1_name+\"_\"+chrom2_name]=read_array\n",
        "    if output_pkl is not None:\n",
        "        output_dir = os.path.dirname(os.path.realpath(output_pkl))\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        write_pkl(output_dict,output_pkl)\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import os \n",
        "    import sys\n",
        "    if len(sys.argv) != 6:\n",
        "        print('Usage: python3 hic2array.py [input.hic] [output.pkl] [resolution] [normalization_type] [mode]')\n",
        "        print(\"This is the full hic2array script. \")\n",
        "        print(\"normalization type: 0: None normalization; 1: VC normalization; 2: VC_SQRT normalization; 3: KR normalization; 4: SCALE normalization\")\n",
        "        print(\"mode: 0 for sparse matrix, 1 for dense matrix, 2 for sparce matrix (only cis-contact); 3 for dense matrix (only cis-contact).\")\n",
        "        sys.exit(1)\n",
        "    resolution = int(sys.argv[3])\n",
        "    normalization_type = int(sys.argv[4])\n",
        "    mode = int(sys.argv[5])\n",
        "    normalization_dict={0:\"NONE\",1:\"VC\",2:\"VC_SQRT\",3:\"KR\",4:\"SCALE\"}\n",
        "    if normalization_type not in normalization_dict:\n",
        "        print('normalization type should be 0,1,2,3,4')\n",
        "        print(\"normalization type: 0: None normalization; 1: VC normalization; 2: VC_SQRT normalization; 3: KR normalization; 4: SCALE normalization\")\n",
        "        sys.exit(1)\n",
        "    normalization_type = normalization_dict[normalization_type]\n",
        "    if mode not in [0,1,2,3]:\n",
        "        print('mode should be in choice of 0/1/2/3')\n",
        "        print(\"mode: 0 for sparse matrix, 1 for dense matrix, 2 for sparce matrix (only cis-contact); 3 for dense matrix (only cis-contact).\")\n",
        "        sys.exit(1)\n",
        "    input_hic_path = os.path.abspath(sys.argv[1])\n",
        "    output_pkl_path = os.path.abspath(sys.argv[2])\n",
        "    output_dir = os.path.dirname(output_pkl_path)\n",
        "    os.makedirs(output_dir,exist_ok=True)\n",
        "    hic2array(input_hic_path,output_pkl_path,resolution,normalization_type,mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18eb0e55-f3fc-453f-b8ad-e2dfb02d65f0",
      "metadata": {},
      "source": [
        "Convert .hic Files to .pkl Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9ac349-4d27-4f34-a449-d52efcbd6deb",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# List available .hic files\n",
        "import glob\n",
        "hic_files = glob.glob('hic-raw/*.hic')\n",
        "print(\"Available .hic files:\")\n",
        "for f in hic_files:\n",
        "    print(f\"  - {f}\")\n",
        "\n",
        "# Convert each file (update filenames as needed)\n",
        "# Example conversions:\n",
        "!python3 utils/hic2array.py hic-raw/Ft1-GSM6077013_at_hic_ndx1-4_r2.hic Ftr1.pkl 25000 0 0\n",
        "!python3 utils/hic2array.py hic-raw/Pt1-GSM4705443_ddcc.hic Ptr1.pkl 25000 0 0\n",
        "!python3 utils/hic2array.py hic-raw/Pt2-GSM6077012_at_hic_ndx1-4_r1.hic Ptr2.pkl 25000 0 0\n",
        "!python3 utils/hic2array.py hic-raw/Pv1-GSM5091844_S_WT_2h1_DNB-15.allValidPairs.hic Pv1.pkl 25000 0 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076f9fda-fe2c-4163-a7c3-cbfd3238ea2c",
      "metadata": {},
      "source": [
        "4. Submatrix Generation\n",
        "Create scan_array.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c577dfa-6646-4919-86ad-d9604e94a538",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%writefile utils/scan_array.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.sparse import coo_matrix\n",
        "import os\n",
        "\n",
        "def write_pickle(output_dict,output_path):\n",
        "    \"\"\"\n",
        "    output_dict: dict, output dictionary\n",
        "    output_path: str, output path\n",
        "    \"\"\"\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(output_dict, f)\n",
        "\n",
        "def scan_matrix(matrix, input_row_size,input_col_size, stride_row,\n",
        "                stride_col,hic_count,output_dir,current_chrom,\n",
        "                filter_threshold=0.05):\n",
        "    \"\"\"\n",
        "    matrix: 2D array\n",
        "    input_row_size: int, row size of scanned output submatrix\n",
        "    input_col_size: int, column size of scanned output submatrix\n",
        "    stride_row: int, row stride\n",
        "    stride_col: int, column stride\n",
        "    hic_count: int, total read count of the Hi-C experiments\n",
        "    output_dir: str, output directory\n",
        "    current_chrom: str, current chromosome\n",
        "    \"\"\"\n",
        "    row_size = matrix.shape[0]\n",
        "    col_size = matrix.shape[1]\n",
        "    count_save=0\n",
        "    region_size = input_row_size * input_col_size\n",
        "    for i in range(0, row_size - input_row_size//2, stride_row):\n",
        "        for j in range(0, col_size - input_col_size//2, stride_col):\n",
        "            submatrix = np.zeros((input_row_size, input_col_size))\n",
        "            row_start = max(0,i)\n",
        "            row_end = min(row_size, i + input_row_size)\n",
        "            col_start = max(0,j)\n",
        "            col_end = min(col_size, j + input_col_size)\n",
        "            submatrix[:row_end-row_start,:col_end-col_start] = matrix[row_start: row_end, col_start: col_end]\n",
        "            #filter out the submatrices with too many zeros\n",
        "            count_useful = np.count_nonzero(submatrix)\n",
        "            if count_useful < region_size * filter_threshold:\n",
        "                continue\n",
        "            \n",
        "            output_dict={}\n",
        "            output_dict['input']=submatrix\n",
        "            output_dict['input_count']=hic_count\n",
        "            #judge if the diag is possibly included\n",
        "            if col_start < row_start and col_end >row_start:\n",
        "                output_dict['diag']=abs (col_start-row_start)\n",
        "            elif col_start == row_start:\n",
        "                output_dict['diag']=0\n",
        "            elif col_start> row_start and col_start < row_end:\n",
        "                output_dict['diag']= -abs (col_start-row_start)\n",
        "            else:\n",
        "                output_dict['diag']=None\n",
        "            output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n",
        "            write_pickle(output_dict,output_path)\n",
        "            count_save+=1\n",
        "            if count_save%100==0:\n",
        "                print('Processed %d submatrices' % count_save, \" for chromosome \", current_chrom)\n",
        "        \n",
        "    return \n",
        "\n",
        "def scan_pickle(input_pkl_path, input_row_size,input_col_size, stride_row,\n",
        "                stride_col,output_dir,filter_threshold):\n",
        "    \"\"\"\n",
        "    input_pkl_path: str, input pickle path  \n",
        "    input_row_size: int, row size of scanned output submatrix\n",
        "    input_col_size: int, column size of scanned output submatrix\n",
        "    stride_row: int, row stride\n",
        "    stride_col: int, column stride\n",
        "    output_dir: str, output directory\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(input_pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    total_count = 0\n",
        "    for key in data:\n",
        "        matrix = data[key]\n",
        "        if isinstance(matrix, np.ndarray):\n",
        "            cur_count = np.sum(matrix)\n",
        "        elif isinstance(matrix, coo_matrix):\n",
        "            cur_count = matrix.sum()\n",
        "        else:\n",
        "            print(\"Type not supported\", type(matrix))\n",
        "            exit()\n",
        "        total_count += cur_count\n",
        "    print(\"Total read count of Hi-C: \", total_count)        \n",
        "\n",
        "    for key in data:\n",
        "        matrix = data[key]\n",
        "        if isinstance(matrix, coo_matrix):\n",
        "            matrix = matrix.toarray()\n",
        "            \n",
        "            if matrix.shape[0]==matrix.shape[1]:\n",
        "                #intra chromosmoe\n",
        "                #get the symmetrical one \n",
        "                upper_tri = np.triu(matrix,1)\n",
        "                all_triu = np.triu(matrix)\n",
        "                matrix = all_triu + upper_tri.T\n",
        "            else:\n",
        "                matrix = matrix\n",
        "        current_chrom = str(key)\n",
        "        if \"chr\" not in current_chrom:\n",
        "            current_chrom = \"chr\" + current_chrom\n",
        "\n",
        "        scan_matrix(matrix, input_row_size,input_col_size, stride_row,\n",
        "                stride_col,total_count,output_dir,current_chrom,filter_threshold)\n",
        "\n",
        "#run with the simple command line\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_pkl_path', type=str, required=True)\n",
        "    parser.add_argument('--input_row_size', type=int, required=True)\n",
        "    parser.add_argument('--input_col_size', type=int, required=True)\n",
        "    parser.add_argument('--stride_row', type=int, required=True)\n",
        "    parser.add_argument('--stride_col', type=int, required=True)\n",
        "    parser.add_argument('--output_dir', type=str, required=True)\n",
        "    parser.add_argument('--filter_threshold', type=float, default=0.05)\n",
        "    args = parser.parse_args()\n",
        "    input_pkl_path = os.path.abspath(args.input_pkl_path)\n",
        "    output_dir = os.path.abspath(args.output_dir)\n",
        "    scan_pickle(input_pkl_path, args.input_row_size, args.input_col_size, \n",
        "                args.stride_row, args.stride_col, output_dir, args.filter_threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f50a9fb-eec5-4e2c-85f0-8b0eeba77eeb",
      "metadata": {},
      "source": [
        "Generate Submatrices for Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c225422-d2aa-4f90-b6cc-3ad75a3e72f7",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Generate submatrices for pre-training\n",
        "!python3 utils/scan_array.py --input_pkl_path Ptr1.pkl  --input_row_size 448 \\\n",
        "    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n",
        "    --output_dir HiC-PTR1 --filter_threshold 0.01\n",
        "\n",
        "!python3 utils/scan_array.py --input_pkl_path Ptr2.pkl  --input_row_size 448 \\\n",
        "    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n",
        "    --output_dir HiC-PTR2 --filter_threshold 0.01\n",
        "\n",
        "!python3 utils/scan_array.py --input_pkl_path Pv1.pkl  --input_row_size 448 \\\n",
        "    --input_col_size 448 --stride_row 224 --stride_col 224 \\\n",
        "    --output_dir HiC-PV1 --filter_threshold 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7c792c-a528-43c4-ac80-92f7dfc15e06",
      "metadata": {},
      "source": [
        "Create Configuration Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a358175f-231e-4d5e-a88f-c31545bfbfc9",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create train.txt\n",
        "with open('input-dirs/pre-train-dirs/train.txt', 'w') as f:\n",
        "    f.write('HiC-PTR1\\n')\n",
        "    f.write('HiC-PTR2\\n')\n",
        "\n",
        "# Create val.txt\n",
        "with open('input-dirs/pre-train-dirs/val.txt', 'w') as f:\n",
        "    f.write('HiC-PV1\\n')\n",
        "\n",
        "print(\"Configuration files created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49deccaa-aea9-43dc-b3de-f11a0971808a",
      "metadata": {},
      "source": [
        "5. Pre-training\n",
        "Run Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d826ea9-5ee3-4403-a208-64ef694e46d0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Note: This will take considerable time\n",
        "!python3 pretrain.py --batch_size 1 --accum_iter 4 \\\n",
        "    --epochs 1 --warmup_epochs 1 --pin_mem \\\n",
        "    --mask_ratio 0.75 --sparsity_ratio 0.05 \\\n",
        "    --blr 1.5e-4 --min_lr 1e-7 --weight_decay 0.05 \\\n",
        "    --model \"vit_large_patch16\" --loss_alpha 1 --seed 888 \\\n",
        "    --data_path \"input-dirs/pre-train-dirs/\" --train_config \"train.txt\" \\\n",
        "    --valid_config \"val.txt\" --output \"hicfoundation_finetune\" \\\n",
        "    --tensorboard 1 --world_size 1 --dist_url \"tcp://localhost:10001\" --rank 0 \\\n",
        "    --input_row_size 448 --input_col_size 448 --patch_size 16 \\\n",
        "    --print_freq 1 --save_freq 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3687e5c4-fd39-4a86-baad-76494e6644c4",
      "metadata": {},
      "source": [
        "Rename Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd43513-7c6b-4f14-8cf3-737c081d2944",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!mv hicfoundation_finetune hicfoundation_pretrain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5498c1cb-42eb-4d71-a51f-4c892a44e6eb",
      "metadata": {},
      "source": [
        "6. Fine-tuning Preparation\n",
        "Create downsample_pkl.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83c6332-f209-4e22-81ed-49b338048ef8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%writefile utils/downsample_pkl.py\n",
        "import sys\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "def array_to_coo(array):\n",
        "    \"\"\"\n",
        "    Convert a regular 2D NumPy array to a scipy.sparse.coo_matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - array (numpy.ndarray): The input 2D array.\n",
        "\n",
        "    Returns:\n",
        "    - scipy.sparse.coo_matrix: The converted COO matrix.\n",
        "    \"\"\"\n",
        "    # Find the non-zero elements in the array\n",
        "    row, col = np.nonzero(array)\n",
        "\n",
        "    # Get the values of the non-zero elements\n",
        "    data = array[row, col]\n",
        "\n",
        "    # Create the COO matrix\n",
        "    coo_mat = coo_matrix((data, (row, col)), shape=array.shape)\n",
        "\n",
        "    return coo_mat\n",
        "\n",
        "def sparse2tag(coo_mat):\n",
        "    tag_len = coo_mat.sum()\n",
        "    tag_len = int(tag_len)\n",
        "    tag_mat = np.zeros((tag_len, 2))\n",
        "    tag_mat = tag_mat.astype(int)\n",
        "    row, col, data = coo_mat.row, coo_mat.col, coo_mat.data\n",
        "    start_idx = 0\n",
        "    for i in range(len(row)):\n",
        "        end_idx = start_idx + int(data[i])\n",
        "        tag_mat[start_idx:end_idx, :] = (row[i], col[i])\n",
        "        start_idx = end_idx\n",
        "    return tag_mat, tag_len\n",
        "\n",
        "def tag2sparse(tag, nsize):\n",
        "    \"\"\"\n",
        "    Coverts a coo-based tag matrix to sparse matrix.\n",
        "    \"\"\"\n",
        "    coo_data, data = np.unique(tag, axis=0, return_counts=True)\n",
        "    row, col = coo_data[:, 0], coo_data[:, 1]\n",
        "    sparse_mat = coo_matrix((data, (row, col)), shape=(nsize, nsize))\n",
        "    return sparse_mat\n",
        "\n",
        "def downsampling_sparce(matrix, down_ratio, verbose=False):\n",
        "    \"\"\"\n",
        "    Downsampling method for sparse matrix.\n",
        "    \"\"\"\n",
        "    if verbose: print(f\"[Downsampling] Matrix shape is {matrix.shape}\")\n",
        "    tag_mat, tag_len = sparse2tag(matrix)\n",
        "    sample_idx = np.random.choice(tag_len, int(tag_len *down_ratio))\n",
        "    sample_tag = tag_mat[sample_idx]\n",
        "    if verbose: print(f'[Downsampling] Sampling {down_ratio} of {tag_len} reads')\n",
        "    down_mat = tag2sparse(sample_tag, matrix.shape[0])\n",
        "    return down_mat\n",
        "\n",
        "\n",
        "def downsample_pkl(input_pkl, output_pkl, downsample_rate):\n",
        "    data = pickle.load(open(input_pkl, 'rb'))\n",
        "    return_dict={}\n",
        "    for chrom in data:\n",
        "        current_data = data[chrom]\n",
        "        if current_data.shape[0] <=100:\n",
        "            continue\n",
        "        #if it is numpy array convert to sparse matrix\n",
        "        if isinstance(current_data, np.ndarray):\n",
        "            current_data = array_to_coo(current_data)\n",
        "            \n",
        "        downsampled_data = downsampling_sparce(current_data, downsample_rate,verbose=1)\n",
        "        return_dict[chrom] = downsampled_data\n",
        "    pickle.dump(return_dict, open(output_pkl, \"wb\"))\n",
        "    print(\"finish downsampling %s\"%output_pkl)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv)!=4:\n",
        "        print(\"Usage: python3 downsample_pkl.py [input.pkl] [output.pkl] [downsample_rate]\")\n",
        "        print(\"This script is used to downsample the input pickle file.\")\n",
        "        print(\"[input.pkl]: the input pickle file\")\n",
        "        print(\"[output.pkl]: the output pickle file\")\n",
        "        print(\"[downsample_rate]: the downsample rate [float].\")\n",
        "        sys.exit(1)\n",
        "    input_pkl = os.path.abspath(sys.argv[1])\n",
        "    output_pkl = os.path.abspath(sys.argv[2])\n",
        "    output_dir = os.path.dirname(output_pkl)\n",
        "    os.makedirs(output_dir, exist_ok=True)    \n",
        "    downsample_rate = float(sys.argv[3])\n",
        "    downsample_pkl(input_pkl, output_pkl, downsample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de963b66-4890-42f9-a1a7-94c7b41a1a86",
      "metadata": {},
      "source": [
        "Downsample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695939e1-5570-42c7-a2d1-0eed2c8e5fdc",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!python3 utils/downsample_pkl.py Ftr1.pkl Ftr1_downsampled.pkl 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d001d4-b026-408b-bbef-b73997165117",
      "metadata": {},
      "source": [
        "Create scan_array_diag.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b23454e-a318-4fd6-99a8-b38a2bd60bed",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%writefile utils/scan_array_diag.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.sparse import coo_matrix\n",
        "import os\n",
        "\n",
        "def write_pickle(output_dict,output_path):\n",
        "    \"\"\"\n",
        "    output_dict: dict, output dictionary\n",
        "    output_path: str, output path\n",
        "    \"\"\"\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(output_dict, f)\n",
        "\n",
        "def scan_matrix_paired(original_matrix, downsampled_matrix, input_row_size, input_col_size, stride,\n",
        "                      hic_count, output_dir, current_chrom):\n",
        "    \"\"\"\n",
        "    original_matrix: 2D array, original high-quality Hi-C matrix\n",
        "    downsampled_matrix: 2D array, downsampled low-quality Hi-C matrix\n",
        "    input_row_size: int, row size of scanned output submatrix\n",
        "    input_col_size: int, column size of scanned output submatrix\n",
        "    stride: int, row stride\n",
        "    hic_count: int, total read count of the Hi-C experiments\n",
        "    output_dir: str, output directory\n",
        "    current_chrom: str, current chromosome\n",
        "    \"\"\"\n",
        "    row_size = original_matrix.shape[0]\n",
        "    col_size = original_matrix.shape[1]\n",
        "    count_save = 0\n",
        "    \n",
        "    # Ensure both matrices have the same dimensions\n",
        "    assert original_matrix.shape == downsampled_matrix.shape, \\\n",
        "        f\"Matrix shapes don't match: {original_matrix.shape} vs {downsampled_matrix.shape}\"\n",
        "    \n",
        "    print(f\"Scanning matrix {current_chrom} with shape {original_matrix.shape}\")\n",
        "    print(f\"Submatrix size: {input_row_size}x{input_col_size}, stride: {stride}\")\n",
        "    \n",
        "    # For rectangular matrices, scan with different patterns\n",
        "    if row_size == col_size:\n",
        "        # Square matrix: use diagonal scanning\n",
        "        for i in range(0, row_size - input_row_size + 1, stride):\n",
        "            j = i  # Diagonal scanning\n",
        "            if j + input_col_size > col_size:\n",
        "                continue\n",
        "                \n",
        "            original_submatrix = original_matrix[i:i+input_row_size, j:j+input_col_size]\n",
        "            downsampled_submatrix = downsampled_matrix[i:i+input_row_size, j:j+input_col_size]\n",
        "            \n",
        "            # Filter out submatrices with too many zeros\n",
        "            count_useful = np.count_nonzero(original_submatrix)\n",
        "            if count_useful < 1:\n",
        "                continue\n",
        "            \n",
        "            # Create paired output dictionary\n",
        "            output_dict = {}\n",
        "            output_dict['input'] = downsampled_submatrix.copy()\n",
        "            output_dict['2d_target'] = original_submatrix.copy()\n",
        "            output_dict['input_count'] = hic_count\n",
        "            \n",
        "            output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n",
        "            write_pickle(output_dict, output_path)\n",
        "            count_save += 1\n",
        "            \n",
        "            if count_save % 100 == 0:\n",
        "                print('Processed %d paired submatrices' % count_save, \" for chromosome \", current_chrom)\n",
        "    else:\n",
        "        # Rectangular matrix: scan all possible positions\n",
        "        for i in range(0, row_size - input_row_size + 1, stride):\n",
        "            for j in range(0, col_size - input_col_size + 1, stride):\n",
        "                original_submatrix = original_matrix[i:i+input_row_size, j:j+input_col_size]\n",
        "                downsampled_submatrix = downsampled_matrix[i:i+input_row_size, j:j+input_col_size]\n",
        "                \n",
        "                # Filter out submatrices with too many zeros\n",
        "                count_useful = np.count_nonzero(original_submatrix)\n",
        "                if count_useful < 1:\n",
        "                    continue\n",
        "                \n",
        "                # Create paired output dictionary\n",
        "                output_dict = {}\n",
        "                output_dict['input'] = downsampled_submatrix.copy()\n",
        "                output_dict['2d_target'] = original_submatrix.copy()\n",
        "                output_dict['input_count'] = hic_count\n",
        "                \n",
        "                output_path = os.path.join(output_dir, str(current_chrom) + '_' + str(i) + '_' + str(j) + '.pkl')\n",
        "                write_pickle(output_dict, output_path)\n",
        "                count_save += 1\n",
        "                \n",
        "                if count_save % 100 == 0:\n",
        "                    print('Processed %d paired submatrices' % count_save, \" for chromosome \", current_chrom)\n",
        "    \n",
        "    print(f\"Total submatrices saved for {current_chrom}: {count_save}\")\n",
        "    return \n",
        "\n",
        "def scan_pickle_paired(original_pkl_path, downsampled_pkl_path, input_row_size, input_col_size, \n",
        "                      stride, output_dir):\n",
        "    \"\"\"\n",
        "    original_pkl_path: str, path to original (high-quality) pickle file\n",
        "    downsampled_pkl_path: str, path to downsampled (low-quality) pickle file  \n",
        "    input_row_size: int, row size of scanned output submatrix\n",
        "    input_col_size: int, column size of scanned output submatrix\n",
        "    stride: int, row stride\n",
        "    output_dir: str, output directory\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load both pickle files\n",
        "    with open(original_pkl_path, 'rb') as f:\n",
        "        original_data = pickle.load(f)\n",
        "    \n",
        "    with open(downsampled_pkl_path, 'rb') as f:\n",
        "        downsampled_data = pickle.load(f)\n",
        "    \n",
        "    # Ensure both datasets have the same chromosomes\n",
        "    assert set(original_data.keys()) == set(downsampled_data.keys()), \\\n",
        "        \"Original and downsampled data must have the same chromosomes\"\n",
        "    \n",
        "    # Calculate total count from original data\n",
        "    total_count = 0\n",
        "    for key in original_data:\n",
        "        matrix = original_data[key]\n",
        "        if isinstance(matrix, np.ndarray):\n",
        "            cur_count = np.sum(matrix)\n",
        "        elif isinstance(matrix, coo_matrix):\n",
        "            cur_count = matrix.sum()\n",
        "        else:\n",
        "            print(\"Type not supported\", type(matrix))\n",
        "            exit()\n",
        "       total_count += cur_count\n",
        "   print(\"Total read count of original Hi-C: \", total_count)        \n",
        "\n",
        "   # Process each chromosome\n",
        "   for key in original_data:\n",
        "       original_matrix = original_data[key]\n",
        "       downsampled_matrix = downsampled_data[key]\n",
        "       \n",
        "       # Convert sparse matrices to dense arrays\n",
        "       if isinstance(original_matrix, coo_matrix):\n",
        "           original_matrix = original_matrix.toarray()\n",
        "       \n",
        "       if isinstance(downsampled_matrix, coo_matrix):\n",
        "           downsampled_matrix = downsampled_matrix.toarray()\n",
        "       \n",
        "       current_chrom = str(key)\n",
        "       if \"chr\" not in current_chrom:\n",
        "           current_chrom = \"chr\" + current_chrom\n",
        "       \n",
        "       # Only apply symmetry operation if matrix is square\n",
        "       if original_matrix.shape[0] == original_matrix.shape[1]:\n",
        "           # Get the symmetrical matrix for square matrices\n",
        "           upper_tri = np.triu(original_matrix, 1)\n",
        "           all_triu = np.triu(original_matrix)\n",
        "           original_matrix = all_triu + upper_tri.T\n",
        "           \n",
        "           upper_tri = np.triu(downsampled_matrix, 1)\n",
        "           all_triu = np.triu(downsampled_matrix)\n",
        "           downsampled_matrix = all_triu + upper_tri.T\n",
        "       else:\n",
        "           print(f\"Warning: Matrix for {current_chrom} is not square ({original_matrix.shape}). Skipping symmetry operation.\")\n",
        "\n",
        "       print(f\"Processing chromosome {current_chrom}\")\n",
        "       print(f\"Original matrix shape: {original_matrix.shape}\")\n",
        "       print(f\"Downsampled matrix shape: {downsampled_matrix.shape}\")\n",
        "\n",
        "       scan_matrix_paired(original_matrix, downsampled_matrix, input_row_size, input_col_size, \n",
        "                         stride, total_count, output_dir, current_chrom)\n",
        "\n",
        "# Run with the simple command line\n",
        "if __name__ == '__main__':\n",
        "   import argparse\n",
        "   parser = argparse.ArgumentParser()\n",
        "   parser.add_argument('--original_pkl_path', type=str, required=True, \n",
        "                      help='Path to original (high-quality) pickle file')\n",
        "   parser.add_argument('--downsampled_pkl_path', type=str, required=True,\n",
        "                      help='Path to downsampled (low-quality) pickle file')\n",
        "   parser.add_argument('--input_row_size', type=int, required=True)\n",
        "   parser.add_argument('--input_col_size', type=int, required=True)\n",
        "   parser.add_argument('--stride', type=int, required=True)\n",
        "   parser.add_argument('--output_dir', type=str, required=True)\n",
        "   args = parser.parse_args()\n",
        "   \n",
        "   original_pkl_path = os.path.abspath(args.original_pkl_path)\n",
        "   downsampled_pkl_path = os.path.abspath(args.downsampled_pkl_path)\n",
        "   output_dir = os.path.abspath(args.output_dir)\n",
        "   \n",
        "   scan_pickle_paired(original_pkl_path, downsampled_pkl_path, args.input_row_size, \n",
        "                     args.input_col_size, args.stride, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc3abcfd-088b-4640-a703-c219d7340c44",
      "metadata": {},
      "source": [
        "Generate Paired Submatrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a9d952b-fc9b-4cc0-8adc-3e9894ecd941",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!python3 utils/scan_array_diag.py \\\n",
        "    --original_pkl_path Ftr1.pkl \\\n",
        "    --downsampled_pkl_path Ftr1_downsampled.pkl \\\n",
        "    --input_row_size 224 --input_col_size 224 --stride 20 \\\n",
        "    --output_dir Ftr1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a935ec-e336-4364-9879-8d501eea26f2",
      "metadata": {},
      "source": [
        "Prepare Fine-tuning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e47e8f82-e03c-4fd0-94ed-1f1765d28be2",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Get all pkl files from Ftr1 directory\n",
        "ftr1_files = glob.glob('Ftr1/*.pkl')\n",
        "\n",
        "# Shuffle and split (80-20 split)\n",
        "random.shuffle(ftr1_files)\n",
        "split_idx = int(0.8 * len(ftr1_files))\n",
        "\n",
        "train_files = ftr1_files[:split_idx]\n",
        "val_files = ftr1_files[split_idx:]\n",
        "\n",
        "# Copy files to respective directories\n",
        "for f in train_files:\n",
        "    shutil.copy(f, 'ft-inputs/train/')\n",
        "for f in val_files:\n",
        "    shutil.copy(f, 'ft-inputs/val/')\n",
        "\n",
        "# Create configuration files\n",
        "with open('ft-inputs/train_config.txt', 'w') as f:\n",
        "    f.write('train\\n')\n",
        "\n",
        "with open('ft-inputs/val_config.txt', 'w') as f:\n",
        "    f.write('val\\n')\n",
        "\n",
        "print(f\"Created fine-tuning dataset: {len(train_files)} train, {len(val_files)} validation samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663324c2-6209-44f2-94fb-e3ef06dbe738",
      "metadata": {},
      "source": [
        "7. Fine-tuning\n",
        "Create Modified train_epoch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71659f16-be8e-4aab-8e33-e0f2b0043e9e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%writefile finetune/train_epoch.py\n",
        "import math\n",
        "import sys\n",
        "import numpy as np\n",
        "from typing import Iterable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "from ops.Logger import MetricLogger,SmoothedValue\n",
        "import model.lr_sched as lr_sched\n",
        "from finetune.loss import configure_loss\n",
        "from ops.train_utils import list_to_device, to_value, create_image, torch_to_nparray, convert_gray_rgbimage\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader_train, optimizer, \n",
        "                loss_scaler, epoch, device,\n",
        "                log_writer=None, args=None):\n",
        "    model.train()\n",
        "    metric_logger = MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = args.print_freq\n",
        "\n",
        "    accum_iter = args.accum_iter\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    if log_writer is not None:\n",
        "        print('Tensorboard log dir: {}'.format(log_writer.log_dir))\n",
        "    print(\"number of iterations: \",len(data_loader_train))\n",
        "    criterion = configure_loss(args)\n",
        "\n",
        "    num_iter = len(data_loader_train)\n",
        "    for data_iter_step, train_data in enumerate(metric_logger.log_every(data_loader_train, print_freq, header)):\n",
        "        if data_iter_step % accum_iter == 0:\n",
        "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader_train) + epoch, args)\n",
        "        input_matrix, total_count, target_matrix, embed_target, target_vector = list_to_device(train_data,device=device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output_embedding, output_2d, output_1d = model(input_matrix, total_count)\n",
        "        \n",
        "        # Calculate losses - ensure all outputs participate in loss calculation\n",
        "        loss_components = []\n",
        "        \n",
        "        if embed_target is not None:\n",
        "            embedding_loss = criterion(output_embedding, embed_target)\n",
        "            loss_components.append(embedding_loss)\n",
        "        else:\n",
        "            # Use a small multiplier on the output to ensure gradients flow\n",
        "            # but don't affect the actual loss value\n",
        "            embedding_loss = 0.0 * output_embedding.mean()\n",
        "            loss_components.append(embedding_loss)\n",
        "            \n",
        "        if target_matrix is not None:\n",
        "            #flatten 2d matrix\n",
        "            output_2d_flatten = torch.flatten(output_2d, start_dim=1,end_dim=-1)\n",
        "            target_matrix_flatten = torch.flatten(target_matrix, start_dim=1,end_dim=-1)\n",
        "            output_2d_loss = criterion(output_2d_flatten, target_matrix_flatten)\n",
        "            loss_components.append(output_2d_loss)\n",
        "        else:\n",
        "            # Use a small multiplier on the output to ensure gradients flow\n",
        "            output_2d_loss = 0.0 * output_2d.mean()\n",
        "            loss_components.append(output_2d_loss)\n",
        "            \n",
        "        if target_vector is not None:\n",
        "            output_1d_loss = criterion(output_1d, target_vector)\n",
        "            loss_components.append(output_1d_loss)\n",
        "        else:\n",
        "            # Use a small multiplier on the output to ensure gradients flow\n",
        "            output_1d_loss = 0.0 * output_1d.mean()\n",
        "            loss_components.append(output_1d_loss)\n",
        "        \n",
        "        # Sum all loss components\n",
        "        loss = sum(loss_components)\n",
        "        \n",
        "        # Update metrics\n",
        "        metric_logger.update(loss=to_value(loss))\n",
        "        metric_logger.update(embedding_loss=to_value(embedding_loss))\n",
        "        metric_logger.update(output_2d_loss=to_value(output_2d_loss))\n",
        "        metric_logger.update(output_1d_loss=to_value(output_1d_loss))\n",
        "        \n",
        "        if not math.isfinite(to_value(loss)):\n",
        "            print(\"Loss is {}, stopping training\".format(to_value(loss)))\n",
        "            #sys.exit(1)\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "            \n",
        "        loss = loss / accum_iter\n",
        "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
        "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
        "\n",
        "        if (data_iter_step + 1) % accum_iter == 0:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        torch.cuda.synchronize() # Make sure all gradients are finished computing before moving on\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        metric_logger.update(lr=lr)\n",
        "        \n",
        "\n",
        "        if log_writer is not None and ((data_iter_step + 1) % accum_iter == 0 or data_iter_step==0):\n",
        "            \"\"\" \n",
        "            We use epoch_1000x as the x-axis in tensorboard.\n",
        "            This calibrates different curves when batch size changes.\n",
        "            \"\"\"\n",
        "            epoch_1000x = int((data_iter_step / len(data_loader_train) + epoch) * 1000)\n",
        "            log_writer.add_scalars('Loss/loss', {'train_loss': to_value(loss)}, epoch_1000x)\n",
        "            log_writer.add_scalars('Loss/embedding_loss', {'train_loss': to_value(embedding_loss)}, epoch_1000x)\n",
        "            log_writer.add_scalars('Loss/output_2d_loss', {'train_loss': to_value(output_2d_loss)}, epoch_1000x)\n",
        "            log_writer.add_scalars('Loss/output_1d_loss', {'train_loss': to_value(output_1d_loss)}, epoch_1000x)\n",
        "            log_writer.add_scalars('LR/lr', {'lr': lr}, epoch_1000x)\n",
        "            if ((data_iter_step+1)//accum_iter)%50==0 or data_iter_step==0:\n",
        "                #add visualization for your output and input\n",
        "                new_samples = create_image(input_matrix)\n",
        "                select_num = min(8,len(new_samples))\n",
        "                sample_image = torch_to_nparray(new_samples.clone().detach()[:select_num])\n",
        "                log_writer.add_images('Input_%s'%\"train\", sample_image, epoch_1000x)\n",
        "                output_2d_image = convert_gray_rgbimage(output_2d.clone().detach()[:select_num])\n",
        "                output_2d_image = torch_to_nparray(output_2d_image)\n",
        "                log_writer.add_images('Output_2d_%s'%\"train\", output_2d_image, epoch_1000x)\n",
        "                # for name, param in model.named_parameters():\n",
        "                #     log_writer.add_histogram(name, param, epoch_1000x)\n",
        "                #raise errors, see https://github.com/pytorch/pytorch/issues/91516\n",
        "                #If you want to use this, install tensorboardX \n",
        "                #then change the code in main_worker.py to \"from tensorboardX import SummaryWriter\"\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b66f075-0380-4f66-961b-c5e759d5273b",
      "metadata": {},
      "source": [
        "Run Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148fb5da-a685-4f14-84ab-e6d782e057f8",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!python3 finetune.py --batch_size 1 --accum_iter 4 \\\n",
        "    --epochs 1 --warmup_epochs 0 --pin_mem \\\n",
        "    --blr 1e-3 --min_lr 1e-7 --weight_decay 0.05 \\\n",
        "    --layer_decay 0.75 --model vit_large_patch16 \\\n",
        "    --pretrain hicfoundation_pretrain/model/model_best.pth.tar \\\n",
        "    --finetune 1 --seed 888 \\\n",
        "    --loss_type 1 --data_path \"ft-inputs\" \\\n",
        "    --train_config \"train_config.txt\" \\\n",
        "    --valid_config \"val_config.txt\" \\\n",
        "    --output \"hicfoundation_finetune\" --tensorboard 1 \\\n",
        "    --world_size 1 --dist_url \"tcp://localhost:10001\" --rank 0 \\\n",
        "    --input_row_size 448 --input_col_size 448 --patch_size 16 \\\n",
        "    --print_freq 1 --save_freq 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084d606c-0c13-4de9-b4ff-a8dbb05f3207",
      "metadata": {},
      "source": [
        "8. Inference\n",
        "Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd935ce0-bd40-46ae-9ecc-0fd916aff767",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Update the filename below to match your uploaded test file\n",
        "!python inference.py --batch_size 1 \\\n",
        "    --input hic-raw/B1-GSM4705442_cmt2cmt3.hic \\\n",
        "    --resolution 10000 \\\n",
        "    --task 3 \\\n",
        "    --input_row_size 224 --input_col_size 224 \\\n",
        "    --stride 32 --bound 0 \\\n",
        "    --num_workers 1 \\\n",
        "    --model hicfoundation_finetune/model/model_best.pth.tar \\\n",
        "    --model_path hicfoundation_finetune/model/model_best.pth.tar \\\n",
        "    --output outputs/B1_enhanced"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1241795f-6900-4708-8ba2-8c6fa302b363",
      "metadata": {},
      "source": [
        "Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6712b1a8-d1d3-40e1-a4fe-e9830fb4cd2e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Zip and download results\n",
        "import zipfile\n",
        "\n",
        "zip_filename = 'hicfoundation_results.zip'\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk('outputs'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, '.'))\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_filename)\n",
        "print(f\"Results downloaded as {zip_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9503283-c4dc-4049-813e-1f45ee3912cb",
      "metadata": {},
      "source": [
        "Done! I apologize if there is any errors, this is my first time using Jupyter Notebook. However, all of the commands and steps are exactly what I did to get my results in the final paper and presentation. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
